{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qyLzP66ifpN"
      },
      "source": [
        "#### pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zILc9yLdKFZG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transform\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchsummary import summary\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "# !pip install ptflops\n",
        "# from ptflops import get_model_complexity_info\n",
        "# !pip install wget\n",
        "# import wget\n",
        "# !pip install requests gdown\n",
        "# import gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c-2wjpZ9vB9Y"
      },
      "outputs": [],
      "source": [
        "# Hyper-parameter and setting\n",
        "image_size = 224\n",
        "batch_size = 16\n",
        "channel_size = 3\n",
        "lr = 1e-3\n",
        "num_epochs_student = 1\n",
        "num_classes = 10\n",
        "\n",
        "# var init\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.manual_seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weWMUElLJSZ2",
        "outputId": "2ebab86d-f480-44e8-8b3d-f3223cbfea39"
      },
      "outputs": [],
      "source": [
        "# Data pre-processing of source and target domain\n",
        "tf_source = transform.Compose([\n",
        "    transform.Resize(image_size),\n",
        "    transform.ToTensor(),\n",
        "    transform.Normalize(mean=[0.1307,],  # Normalization as MNIST setting\n",
        "                        std=[0.30,],\n",
        "                        )\n",
        "])\n",
        "\n",
        "tf_target = transform.Compose([\n",
        "    transform.Resize(image_size),\n",
        "    transform.ToTensor(),\n",
        "    transform.Normalize(mean=[0.5, 0.5, 0.5],  # Normalization as default setting\n",
        "                        std=[0.5, 0.5, 0.5],\n",
        "                        )\n",
        "])\n",
        "\n",
        "########################################################################################\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Dataset and dataloader source & target domain (train_set)\n",
        "# Ds_source = datasets.MNIST(root='Dataset/mnist_dataset/', download=True, train=True, transform=tf_source)\n",
        "# Dl_source = DataLoader(Ds_source, batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# Download here: https://drive.google.com/drive/folders/14IgFgyHyJ3I8-VPzzA33sBF1w2E1mVVt?usp=sharing\n",
        "# Mount and unzip from directory\n",
        "# from google.colab import drive\n",
        "# drive.mount('./drive/')\n",
        "# !unzip './drive/MyDrive/99H_datasets/MNIST_M.zip'\n",
        "# Upload directly\n",
        "# !unzip './MNIST_M.zip'\n",
        "\n",
        "# Ds_target = datasets.ImageFolder(root='./MNIST_M/train', transform=tf_target)\n",
        "# Dl_target = DataLoader(Ds_target, batch_size, shuffle=True, num_workers=2)\n",
        "# train_split = 0.8\n",
        "# train_size = int(train_split * len(Dl_target))\n",
        "# test_size = len(Dl_target) - train_size\n",
        "# Dl_tar_training_set, Dl_tar_testing_set = torch.utils.data.random_split(Dl_target, [train_size, test_size])\n",
        "Ds_target_train = datasets.ImageFolder(root='/home/crueang/Chaks/AIOT/data/MNIST_M/train', transform=tf_target)\n",
        "Dl_tar_training_set = DataLoader(Ds_target_train, batch_size, shuffle=True, num_workers=2)\n",
        "Ds_target_test = datasets.ImageFolder(root='/home/crueang/Chaks/AIOT/data/MNIST_M/test', transform=tf_target)\n",
        "Dl_tar_testing_set = DataLoader(Ds_target_test, batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "def plot_graph(history):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_figwidth(10)\n",
        "    fig.suptitle(\"Train vs Validation\")\n",
        "    ax1.plot(history[\"train_student_acc\"], label=\"Train\")\n",
        "    ax1.plot(history[\"val_student_acc\"], label=\"Validation\")\n",
        "    ax1.legend()\n",
        "    ax1.set_title(\"Accuracy\")\n",
        "\n",
        "    ax2.plot(history[\"train_student_loss\"], label=\"Train\")\n",
        "    ax2.plot(history[\"val_student_loss\"], label=\"Validation\")\n",
        "    ax2.legend()\n",
        "    ax2.set_title(\"Loss\")\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "7dZ0ASNwK9nM"
      },
      "outputs": [],
      "source": [
        "# @title FLOPS computation\n",
        "# Code from https://github.com/Eric-mingjie/rethinking-network-pruning/blob/master/imagenet/l1-norm-pruning/compute_flops.py\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def print_model_param_nums(model=None):\n",
        "    if model == None:\n",
        "        model = torchvision.models.alexnet()\n",
        "    total = sum([param.nelement() if param.requires_grad else 0 for param in model.parameters()])\n",
        "    print('  + Number of params: %.4fM' % (total / 1e6))\n",
        "\n",
        "def count_model_param_flops(model=None, input_res=224, multiply_adds=True):\n",
        "\n",
        "    prods = {}\n",
        "    def save_hook(name):\n",
        "        def hook_per(self, input, output):\n",
        "            prods[name] = np.prod(input[0].shape)\n",
        "        return hook_per\n",
        "\n",
        "    list_1=[]\n",
        "    def simple_hook(self, input, output):\n",
        "        list_1.append(np.prod(input[0].shape))\n",
        "    list_2={}\n",
        "    def simple_hook2(self, input, output):\n",
        "        list_2['names'] = np.prod(input[0].shape)\n",
        "\n",
        "\n",
        "    list_conv=[]\n",
        "    def conv_hook(self, input, output):\n",
        "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
        "        output_channels, output_height, output_width = output[0].size()\n",
        "\n",
        "        kernel_ops = self.kernel_size[0] * self.kernel_size[1] * (self.in_channels / self.groups)\n",
        "        bias_ops = 1 if self.bias is not None else 0\n",
        "\n",
        "        params = output_channels * (kernel_ops + bias_ops)\n",
        "        # flops = (kernel_ops * (2 if multiply_adds else 1) + bias_ops) * output_channels * output_height * output_width * batch_size\n",
        "\n",
        "        num_weight_params = (self.weight.data != 0).float().sum()\n",
        "        flops = (num_weight_params * (2 if multiply_adds else 1) + bias_ops * output_channels) * output_height * output_width * batch_size\n",
        "\n",
        "        list_conv.append(flops)\n",
        "\n",
        "    list_linear=[]\n",
        "    def linear_hook(self, input, output):\n",
        "        batch_size = input[0].size(0) if input[0].dim() == 2 else 1\n",
        "\n",
        "        weight_ops = self.weight.nelement() * (2 if multiply_adds else 1)\n",
        "        bias_ops = self.bias.nelement()\n",
        "\n",
        "        flops = batch_size * (weight_ops + bias_ops)\n",
        "        list_linear.append(flops)\n",
        "\n",
        "    list_bn=[]\n",
        "    def bn_hook(self, input, output):\n",
        "        list_bn.append(input[0].nelement() * 2)\n",
        "\n",
        "    list_relu=[]\n",
        "    def relu_hook(self, input, output):\n",
        "        list_relu.append(input[0].nelement())\n",
        "\n",
        "    list_pooling=[]\n",
        "    def pooling_hook(self, input, output):\n",
        "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
        "        output_channels, output_height, output_width = output[0].size()\n",
        "\n",
        "        kernel_ops = self.kernel_size * self.kernel_size\n",
        "        bias_ops = 0\n",
        "        params = 0\n",
        "        flops = (kernel_ops + bias_ops) * output_channels * output_height * output_width * batch_size\n",
        "\n",
        "        list_pooling.append(flops)\n",
        "\n",
        "    list_upsample=[]\n",
        "\n",
        "    # For bilinear upsample\n",
        "    def upsample_hook(self, input, output):\n",
        "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
        "        output_channels, output_height, output_width = output[0].size()\n",
        "\n",
        "        flops = output_height * output_width * output_channels * batch_size * 12\n",
        "        list_upsample.append(flops)\n",
        "\n",
        "    def foo(net):\n",
        "        childrens = list(net.children())\n",
        "        if not childrens:\n",
        "            if isinstance(net, torch.nn.Conv2d):\n",
        "                net.register_forward_hook(conv_hook)\n",
        "            if isinstance(net, torch.nn.Linear):\n",
        "                net.register_forward_hook(linear_hook)\n",
        "            if isinstance(net, torch.nn.BatchNorm2d):\n",
        "                net.register_forward_hook(bn_hook)\n",
        "            if isinstance(net, torch.nn.ReLU):\n",
        "                net.register_forward_hook(relu_hook)\n",
        "            if isinstance(net, torch.nn.MaxPool2d) or isinstance(net, torch.nn.AvgPool2d):\n",
        "                net.register_forward_hook(pooling_hook)\n",
        "            if isinstance(net, torch.nn.Upsample):\n",
        "                net.register_forward_hook(upsample_hook)\n",
        "            return\n",
        "        for c in childrens:\n",
        "            foo(c)\n",
        "\n",
        "    if model == None:\n",
        "        model = torchvision.models.alexnet()\n",
        "    foo(model)\n",
        "    input = Variable(torch.rand(3,input_res,input_res).unsqueeze(0), requires_grad = True)\n",
        "    out = model(input)\n",
        "\n",
        "\n",
        "    total_flops = (sum(list_conv) + sum(list_linear) + sum(list_bn) + sum(list_relu) + sum(list_pooling) + sum(list_upsample))\n",
        "\n",
        "    print('Number of FLOPs: %.6f GFLOPs (%.2f MFLOPs)' % (total_flops / 1e9, total_flops / 1e6))\n",
        "\n",
        "    return total_flops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ptsq_VGG_model(nn.Module):\n",
        "    def __init__(self, n_C=10):\n",
        "        super(ptsq_VGG_model, self).__init__()\n",
        "        # QuantStub converts tensors from floating point to quantized\n",
        "        self.quant = torch.ao.quantization.QuantStub()\n",
        "\n",
        "        self.VGG = torchvision.models.vgg16(pretrained=True)\n",
        "        self.VGG.classifier[-1] = nn.Linear(in_features=4096, out_features=10)\n",
        "        # DeQuantStub converts tensors from quantized to floating point\n",
        "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Handle single-channel input by expanding (repeating) the singleton dimension\n",
        "        # MNIST (grayscale, 1-channel) to 3-channel\n",
        "        x = x.expand(x.data.shape[0], channel_size, image_size, image_size).to(device)\n",
        "        # print(x.shape)\n",
        "        x = self.quant(x)\n",
        "        \n",
        "        x = self.VGG(x)\n",
        "        \n",
        "        x = self.dequant(x)\n",
        "        x = self.logsoftmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt7RKA8atQD0",
        "outputId": "6e839b9f-33ad-4319-d5b3-7c7552fb77d6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/crueang/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/home/crueang/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "         QuantStub-1          [-1, 3, 224, 224]               0\n",
            "            Conv2d-2         [-1, 64, 224, 224]           1,792\n",
            "              ReLU-3         [-1, 64, 224, 224]               0\n",
            "            Conv2d-4         [-1, 64, 224, 224]          36,928\n",
            "              ReLU-5         [-1, 64, 224, 224]               0\n",
            "         MaxPool2d-6         [-1, 64, 112, 112]               0\n",
            "            Conv2d-7        [-1, 128, 112, 112]          73,856\n",
            "              ReLU-8        [-1, 128, 112, 112]               0\n",
            "            Conv2d-9        [-1, 128, 112, 112]         147,584\n",
            "             ReLU-10        [-1, 128, 112, 112]               0\n",
            "        MaxPool2d-11          [-1, 128, 56, 56]               0\n",
            "           Conv2d-12          [-1, 256, 56, 56]         295,168\n",
            "             ReLU-13          [-1, 256, 56, 56]               0\n",
            "           Conv2d-14          [-1, 256, 56, 56]         590,080\n",
            "             ReLU-15          [-1, 256, 56, 56]               0\n",
            "           Conv2d-16          [-1, 256, 56, 56]         590,080\n",
            "             ReLU-17          [-1, 256, 56, 56]               0\n",
            "        MaxPool2d-18          [-1, 256, 28, 28]               0\n",
            "           Conv2d-19          [-1, 512, 28, 28]       1,180,160\n",
            "             ReLU-20          [-1, 512, 28, 28]               0\n",
            "           Conv2d-21          [-1, 512, 28, 28]       2,359,808\n",
            "             ReLU-22          [-1, 512, 28, 28]               0\n",
            "           Conv2d-23          [-1, 512, 28, 28]       2,359,808\n",
            "             ReLU-24          [-1, 512, 28, 28]               0\n",
            "        MaxPool2d-25          [-1, 512, 14, 14]               0\n",
            "           Conv2d-26          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-27          [-1, 512, 14, 14]               0\n",
            "           Conv2d-28          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-29          [-1, 512, 14, 14]               0\n",
            "           Conv2d-30          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-31          [-1, 512, 14, 14]               0\n",
            "        MaxPool2d-32            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-33            [-1, 512, 7, 7]               0\n",
            "           Linear-34                 [-1, 4096]               0\n",
            "             ReLU-35                 [-1, 4096]               0\n",
            "          Dropout-36                 [-1, 4096]               0\n",
            "           Linear-37                 [-1, 4096]               0\n",
            "             ReLU-38                 [-1, 4096]               0\n",
            "          Dropout-39                 [-1, 4096]               0\n",
            "           Linear-40                   [-1, 10]               0\n",
            "              VGG-41                   [-1, 10]               0\n",
            "      DeQuantStub-42                   [-1, 10]               0\n",
            "       LogSoftmax-43                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 219.92\n",
            "Params size (MB): 56.13\n",
            "Estimated Total Size (MB): 276.63\n",
            "----------------------------------------------------------------\n",
            "Number of FLOPs: 30.726486 GFLOPs (30726.49 MFLOPs)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor(3.0726e+10)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create a model instance\n",
        "device = 'cpu'\n",
        "model_ptdq_fp32 = ptsq_VGG_model().to(device)\n",
        "# create a quantized model instance\n",
        "model_ptdq_int8 = torch.ao.quantization.quantize_dynamic(\n",
        "    model_ptdq_fp32,  # the original model\n",
        "    {torch.nn.Linear},  # a set of layers to dynamically quantize\n",
        "    dtype=torch.qint8)  # the target dtype for quantized weights\n",
        "\n",
        "# run the model\n",
        "input_fp32 = torch.randn(1, 3, 224, 224)\n",
        "res = model_ptdq_int8(input_fp32)\n",
        "summary(model_ptdq_int8, input_size=(channel_size, image_size, image_size))\n",
        "count_model_param_flops(model=model_ptdq_int8.eval(), input_res=image_size, multiply_adds=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8rXT3xcqvL-"
      },
      "source": [
        "#### Post Training Static Quantization (i.e., PTSQ)\n",
        "\n",
        "Post Training Static Quantization (PTQ static) quantizes the weights and activations of the model. It fuses activations into preceding layers where possible. It requires calibration with a representative dataset to determine optimal quantization parameters for activations. Post Training Static Quantization is typically used when both memory bandwidth and compute savings are important with CNNs being a typical use case.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# original model\n",
        "# all tensors and computations are in floating point\n",
        "previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n",
        "                    /\n",
        "    linear_weight_fp32\n",
        "\n",
        "# statically quantized model\n",
        "# weights and activations are in int8\n",
        "previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8\n",
        "                    /\n",
        "  linear_weight_int8\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa7KAW6Yq-ct",
        "outputId": "82922e86-061a-4b1f-fec2-9ecb0eebd32e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/crueang/anaconda3/lib/python3.11/site-packages/torch/ao/quantization/observer.py:221: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "The expanded size of the tensor (224) must match the existing size (28) at non-singleton dimension 3.  Target sizes: [4, 3, 224, 224].  Tensor sizes: [4, 3, 28, 28]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 86\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# calibrate the prepared model to determine quantization parameters for activations\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# in a real world setting, the calibration would be done with a representative dataset\u001b[39;00m\n\u001b[1;32m     85\u001b[0m input_fp32 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)\n\u001b[0;32m---> 86\u001b[0m model_fp32_ptsq_prepared(input_fp32)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Convert the observed model to a quantized model. This does several things:\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# quantizes the weights, computes and stores the scale and bias value to be\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# used with each activation tensor, and replaces key operators with quantized\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# implementations.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m model_ptsq_int8 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mao\u001b[38;5;241m.\u001b[39mquantization\u001b[38;5;241m.\u001b[39mconvert(model_fp32_ptsq_prepared)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[7], line 40\u001b[0m, in \u001b[0;36mptsq_student_model.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Handle single-channel input by expanding (repeating) the singleton dimension\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# MNIST (grayscale, 1-channel) to 3-channel\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mexpand(x\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], channel_size, image_size, image_size)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant(x)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (224) must match the existing size (28) at non-singleton dimension 3.  Target sizes: [4, 3, 224, 224].  Tensor sizes: [4, 3, 28, 28]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# define a floating point model where some layers could be statically quantized\n",
        "class ptsq_student_model(nn.Module):\n",
        "    def __init__(self, n_C=10):\n",
        "        super(ptsq_student_model, self).__init__()\n",
        "        # QuantStub converts tensors from floating point to quantized\n",
        "        self.quant = torch.ao.quantization.QuantStub()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size = 5)\n",
        "        self.batchnorm2d_1 = nn.BatchNorm2d(32)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.maxpool2d_1 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 5)\n",
        "        self.batchnorm2d_2 = nn.BatchNorm2d(64)\n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.maxpool2d_2 = nn.MaxPool2d(2)\n",
        "\n",
        "        # Expand from one extractor to two extractor (classed and domain-classed)\n",
        "        self.num_cnn_features = 64 * 4 * 4  # 1024 flattening\n",
        "\n",
        "        # Label classification (blue section)\n",
        "        self.linear_1 = nn.Linear(self.num_cnn_features, 64)\n",
        "        # self.batchnorm1d = nn.BatchNorm1d(64)\n",
        "        self.relu_lin1 = nn.ReLU()\n",
        "        # self.dropout = nn.Dropout()\n",
        "\n",
        "        # output logits layer (10 classes)\n",
        "        self.linear_2 = nn.Linear(64, n_C)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "        # DeQuantStub converts tensors from quantized to floating point\n",
        "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Handle single-channel input by expanding (repeating) the singleton dimension\n",
        "        # MNIST (grayscale, 1-channel) to 3-channel\n",
        "        x = x.expand(x.data.shape[0], channel_size, image_size, image_size)\n",
        "        # print(x.shape)\n",
        "        x = self.quant(x)\n",
        "        x = self.maxpool2d_1(self.relu_1(self.batchnorm2d_1(self.conv1(x))))\n",
        "        x = self.maxpool2d_2(self.relu_2(self.batchnorm2d_2(self.conv2(x))))\n",
        "        fc = x.reshape(-1, self.num_cnn_features) #Flattening\n",
        "        fc = self.relu_lin1(self.linear_1(fc))  #regular features classification\n",
        "        logits = self.linear_2(fc)\n",
        "        logits = self.dequant(logits)\n",
        "        out = self.logsoftmax(logits)\n",
        "        return out\n",
        "\n",
        "# create a model instance\n",
        "model_fp32_ptsq = ptsq_student_model()\n",
        "\n",
        "# model must be set to eval mode for static quantization logic to work\n",
        "model_fp32_ptsq.eval()\n",
        "\n",
        "# attach a global qconfig, which contains information about what kind\n",
        "# of observers to attach. Use 'x86' for server inference and 'qnnpack'\n",
        "# for mobile inference. Other quantization configurations such as selecting\n",
        "# symmetric or asymmetric quantization and MinMax or L2Norm calibration techniques\n",
        "# can be specified here.\n",
        "# Note: the old 'fbgemm' is still available but 'x86' is the recommended default\n",
        "# for server inference.\n",
        "# model_fp32_ptsq.qconfig = torch.ao.quantization.get_default_qconfig('fbgemm')\n",
        "model_fp32_ptsq.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
        "# model_fp32_ptsq.qconfig = torch.ao.quantization.get_default_qconfig('qnnpack')\n",
        "\n",
        "# Fuse the activations to preceding layers, where applicable.\n",
        "# This needs to be done manually depending on the model architecture.\n",
        "# Common fusions include `conv + relu` and `conv + batchnorm + relu`\n",
        "model_fp32_ptsq_fused = torch.ao.quantization.fuse_modules(model_fp32_ptsq,\n",
        "                                                      [['conv1', 'batchnorm2d_1', 'relu_1'],\n",
        "                                                       ['conv2', 'batchnorm2d_2', 'relu_2'],\n",
        "                                                       ['linear_1', 'relu_lin1'],\n",
        "                                                      #  ['linear_2']\n",
        "                                                       ])\n",
        "\n",
        "# Prepare the model for static quantization. This inserts observers in\n",
        "# the model that will observe activation tensors during calibration.\n",
        "model_fp32_ptsq_prepared = torch.ao.quantization.prepare(model_fp32_ptsq_fused)\n",
        "\n",
        "# calibrate the prepared model to determine quantization parameters for activations\n",
        "# in a real world setting, the calibration would be done with a representative dataset\n",
        "input_fp32 = torch.randn(4, 3, 28, 28)\n",
        "model_fp32_ptsq_prepared(input_fp32)\n",
        "\n",
        "# Convert the observed model to a quantized model. This does several things:\n",
        "# quantizes the weights, computes and stores the scale and bias value to be\n",
        "# used with each activation tensor, and replaces key operators with quantized\n",
        "# implementations.\n",
        "model_ptsq_int8 = torch.ao.quantization.convert(model_fp32_ptsq_prepared)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# run the model, relevant calculations will happen in int8\n",
        "res = model_ptsq_int8(input_fp32)\n",
        "# summary(model_int8, input_size=(channel_size, image_size, image_size))\n",
        "# Print the total number of parameters directly (no need for .numpy() anymore)\n",
        "print(f'int8 ptsq model: {model_ptsq_int8}')\n",
        "print(f'\\n\\n\\nint8 qat model: {model_ptsq_int8}')\n",
        "total_params = sum(p.numel() for p in model_ptsq_int8.parameters())\n",
        "print(\"\\n\\n\\nTotal parameters of Post Training Static Quantization (i.e., PTSQ):\", total_params)\n",
        "count_model_param_flops(model=model_ptsq_int8.eval(), input_res=28, multiply_adds=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syNDEUhWViQA"
      },
      "source": [
        "#### Quantization Aware Training (QAT) for Static Quantization\n",
        "\n",
        "Core idea = Quantization + Fine-Tuning (FT)\\\n",
        "\\\n",
        "Quantization Aware Training (QAT) models the effects of quantization during training allowing for higher accuracy compared to other quantization methods. We can do QAT for static, dynamic or weight only quantization. During training, all calculations are done in floating point, with fake_quant modules modeling the effects of quantization by clamping and rounding to simulate the effects of INT8. After model conversion, weights and activations are quantized, and activations are fused into the preceding layer where possible. It is commonly used with CNNs and yields a higher accuracy compared to static quantization.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# original model\n",
        "# all tensors and computations are in floating point\n",
        "previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n",
        "                      /\n",
        "    linear_weight_fp32\n",
        "\n",
        "# model with fake_quants for modeling quantization numerics during training\n",
        "previous_layer_fp32 -- fq -- linear_fp32 -- activation_fp32 -- fq -- next_layer_fp32\n",
        "                           /\n",
        "   linear_weight_fp32 -- fq\n",
        "\n",
        "# quantized model\n",
        "# weights and activations are in int8\n",
        "previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8\n",
        "                     /\n",
        "   linear_weight_int8\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwtdm4AHVbk8"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "qat_student_model(\n",
            "  (quant): QuantStub()\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (batchnorm2d_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu_1): ReLU()\n",
            "  (maxpool2d_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (batchnorm2d_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu_2): ReLU()\n",
            "  (maxpool2d_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (linear_1): Linear(in_features=1024, out_features=64, bias=True)\n",
            "  (relu_lin1): ReLU()\n",
            "  (linear_2): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (logsoftmax): LogSoftmax(dim=1)\n",
            "  (dequant): DeQuantStub()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class qat_student_model(nn.Module):\n",
        "    def __init__(self, n_C=10):\n",
        "        super(qat_student_model, self).__init__()\n",
        "        # QuantStub converts tensors from floating point to quantized\n",
        "        self.quant = torch.ao.quantization.QuantStub()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size = 5)\n",
        "        self.batchnorm2d_1 = nn.BatchNorm2d(32)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.maxpool2d_1 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 5)\n",
        "        self.batchnorm2d_2 = nn.BatchNorm2d(64)\n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.maxpool2d_2 = nn.MaxPool2d(2)\n",
        "\n",
        "        # Expand from one extractor to two extractor (classed and domain-classed)\n",
        "        self.num_cnn_features = 64 * 4 * 4  # 1024 flattening\n",
        "\n",
        "        # Label classification (blue section)\n",
        "        self.linear_1 = nn.Linear(self.num_cnn_features, 64)\n",
        "        # self.batchnorm1d = nn.BatchNorm1d(64)\n",
        "        self.relu_lin1 = nn.ReLU()\n",
        "        # self.dropout = nn.Dropout()\n",
        "\n",
        "        # output logits layer (10 classes)\n",
        "        self.linear_2 = nn.Linear(64, n_C)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "        # DeQuantStub converts tensors from quantized to floating point\n",
        "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Handle single-channel input by expanding (repeating) the singleton dimension\n",
        "        # MNIST (grayscale, 1-channel) to 3-channel\n",
        "        x = x.expand(x.data.shape[0], channel_size, image_size, image_size)\n",
        "        # print(x.shape)\n",
        "        x = self.quant(x)\n",
        "        x = self.maxpool2d_1(self.relu_1(self.batchnorm2d_1(self.conv1(x))))\n",
        "        x = self.maxpool2d_2(self.relu_2(self.batchnorm2d_2(self.conv2(x))))\n",
        "        fc = x.reshape(-1, self.num_cnn_features) #Flattening\n",
        "        fc = self.relu_lin1(self.linear_1(fc))  #regular features classification\n",
        "        logits = self.linear_2(fc)\n",
        "        logits = self.dequant(logits)\n",
        "        out = self.logsoftmax(logits)\n",
        "        return out\n",
        "\n",
        "s = qat_student_model()\n",
        "print(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SsTY4BRXy3O",
        "outputId": "5513ee8d-9ebb-4d91-e21f-c6d115327370"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/crueang/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/home/crueang/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (4): ReLU(inplace=True)\n",
            "  (5): Dropout(p=0.5, inplace=False)\n",
            "  (6): Linear(in_features=4096, out_features=10, bias=True)\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/crueang/anaconda3/lib/python3.11/site-packages/torch/ao/quantization/observer.py:221: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/home/crueang/anaconda3/lib/python3.11/site-packages/torch/ao/quantization/utils.py:376: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor predicted: tensor([[-3.6161, -0.6161, -1.6161, -3.6161, -3.6161, -3.6161, -3.6161, -2.6161,\n",
            "         -3.6161, -3.6161]]) || dtype: torch.float32\n",
            "\n",
            "\n",
            "\n",
            "int8 qat model: qat_student_model(\n",
            "  (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
            "  (VGG): VGG(\n",
            "    (features): Sequential(\n",
            "      (0): QuantizedConvReLU2d(3, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
            "      (1): Identity()\n",
            "      (2): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
            "      (3): Identity()\n",
            "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (5): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
            "      (6): Identity()\n",
            "      (7): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
            "      (8): Identity()\n",
            "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (10): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
            "      (11): Identity()\n",
            "      (12): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
            "      (13): Identity()\n",
            "      (14): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
            "      (15): Identity()\n",
            "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (17): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
            "      (18): Identity()\n",
            "      (19): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
            "      (20): Identity()\n",
            "      (21): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
            "      (22): Identity()\n",
            "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (24): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
            "      (25): Identity()\n",
            "      (26): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
            "      (27): Identity()\n",
            "      (28): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 1))\n",
            "      (29): Identity()\n",
            "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "    (classifier): Sequential(\n",
            "      (0): QuantizedLinearReLU(in_features=25088, out_features=4096, scale=1.0, zero_point=0, qscheme=torch.per_channel_affine)\n",
            "      (1): Identity()\n",
            "      (2): QuantizedDropout(p=0.5, inplace=False)\n",
            "      (3): QuantizedLinearReLU(in_features=4096, out_features=4096, scale=1.0, zero_point=0, qscheme=torch.per_channel_affine)\n",
            "      (4): Identity()\n",
            "      (5): QuantizedDropout(p=0.5, inplace=False)\n",
            "      (6): QuantizedLinear(in_features=4096, out_features=10, scale=1.0, zero_point=0, qscheme=torch.per_channel_affine)\n",
            "    )\n",
            "  )\n",
            "  (dequant): DeQuantize()\n",
            "  (logsoftmax): LogSoftmax(dim=1)\n",
            ")\n",
            "\n",
            "\n",
            "\n",
            "Total parameters of Quantization-Aware Training (QAT)): 0\n",
            "Number of FLOPs: 0.006121 GFLOPs (6.12 MFLOPs)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "6121472"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# define a floating point model where some layers could be statically quantized\n",
        "class qat_student_model(nn.Module):\n",
        "    def __init__(self, VGG, n_C=10):\n",
        "        super(qat_student_model, self).__init__()\n",
        "        # QuantStub converts tensors from floating point to quantized\n",
        "        self.quant = torch.ao.quantization.QuantStub()\n",
        "\n",
        "        self.VGG = VGG\n",
        "        # DeQuantStub converts tensors from quantized to floating point\n",
        "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Handle single-channel input by expanding (repeating) the singleton dimension\n",
        "        # MNIST (grayscale, 1-channel) to 3-channel\n",
        "        x = x.expand(x.data.shape[0], channel_size, image_size, image_size).to(device)\n",
        "        # print(x.shape)\n",
        "        x = self.quant(x)\n",
        "        \n",
        "        x = self.VGG(x)\n",
        "        \n",
        "        x = self.dequant(x)\n",
        "        x = self.logsoftmax(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "VGG16 = torchvision.models.vgg16(pretrained=True)\n",
        "VGG16.classifier[-1] = nn.Linear(in_features=4096, out_features=10)\n",
        "\n",
        "VGG16 = VGG16.to(device)\n",
        "\n",
        "VGG16.load_state_dict(torch.load('/home/crueang/jira/W6/cp/best_pretrainedmodel.pth', weights_only=True))  # load cp to model\n",
        "\n",
        "VGG16.eval()\n",
        "# create a model instance\n",
        "model_qat_fp32 = qat_student_model(VGG16)\n",
        "\n",
        "# model must be set to eval mode for static quantization logic to work\n",
        "model_qat_fp32.eval()\n",
        "\n",
        "print(model_qat_fp32.VGG.classifier)\n",
        "\n",
        "modules = []\n",
        "for i in range(len(model_qat_fp32.VGG.features)):\n",
        "    if 'Conv' in str(type(model_qat_fp32.VGG.features[i])):\n",
        "        modules.append([f'VGG.features.{i}', f'VGG.features.{i+1}'])\n",
        "        i += 1\n",
        "\n",
        "for i in range(len(model_qat_fp32.VGG.classifier)):\n",
        "    if 'Linear' in str(type(model_qat_fp32.VGG.classifier[i])):\n",
        "        modules.append([f'VGG.classifier.{i}', f'VGG.classifier.{i+1}'])\n",
        "        i += 1\n",
        "\n",
        "modules.pop()\n",
        "\n",
        "# for layer in model_qat_fp32.VGG.features:\n",
        "#     print('VGG.features[0]')\n",
        "\n",
        "# attach a global qconfig, which contains information about what kind\n",
        "# of observers to attach. Use 'x86' for server inference and 'qnnpack'\n",
        "# for mobile inference. Other quantization configurations such as selecting\n",
        "# symmetric or asymmetric quantization and MinMax or L2Norm calibration techniques\n",
        "# can be specified here.\n",
        "# Note: the old 'fbgemm' is still available but 'x86' is the recommended default\n",
        "# for server inference.\n",
        "# model_ptsq_fp32.qconfig = torch.ao.quantization.get_default_qconfig('fbgemm')\n",
        "model_qat_fp32.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\n",
        "# model_qat_fp32.qconfig = torch.ao.quantization.get_default_qconfig('qnnpack')\n",
        "\n",
        "# Fuse the activations to preceding layers, where applicable.\n",
        "# This needs to be done manually depending on the model architecture.\n",
        "# Common fusions include `conv + relu` and `conv + batchnorm + relu`\n",
        "model_qat_fp32_fused = torch.ao.quantization.fuse_modules(model_qat_fp32, modules)\n",
        "\n",
        "# Prepare the model for static quantization. This inserts observers in\n",
        "# the model that will observe activation tensors during calibration.\n",
        "model_qat_fp32_prepared = torch.ao.quantization.prepare_qat(model_qat_fp32_fused.train())\n",
        "\n",
        "# Convert the observed model to a quantized model. This does several things:\n",
        "# quantizes the weights, computes and stores the scale and bias value to be\n",
        "# used with each activation tensor, and replaces key operators with quantized\n",
        "# implementations.\n",
        "model_qat_fp32_prepared.eval()\n",
        "model_qat_int8 = torch.ao.quantization.convert(model_qat_fp32_prepared)\n",
        "\n",
        "torch.save(model_qat_int8.state_dict(), '/home/crueang/jira/W6/cp/qat_int8.pth')\n",
        "\n",
        "# run the model, relevant calculations will happen in int8\n",
        "res = model_qat_int8(input_fp32)\n",
        "print(f'tensor predicted: {res} || dtype: {res.dtype}')\n",
        "# summary(model_int8, input_size=(channel_size, image_size, image_size))\n",
        "# Print the total number of parameters directly (no need for .numpy() anymore)\n",
        "print(f'\\n\\n\\nint8 qat model: {model_qat_int8}')\n",
        "total_params = sum(p.numel() for p in model_qat_int8.parameters())\n",
        "print(\"\\n\\n\\nTotal parameters of Quantization-Aware Training (QAT)):\", total_params)\n",
        "count_model_param_flops(model=model_qat_int8.eval(), input_res=image_size, multiply_adds=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqEPNJaTa80V"
      },
      "source": [
        "-------------\n",
        "# Training + Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmAEKnLLqBhw"
      },
      "source": [
        "#### Student batch running (Non-quantized and Quantization Dynamically, i.e., PTDQ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncxiL3uwvkj6"
      },
      "outputs": [],
      "source": [
        "# Normal training (Non-Quantized)\n",
        "\n",
        "\n",
        "\n",
        "# Optimizer and cost function\n",
        "optimizer = optim.Adam(student_fn.parameters(), lr)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "###########################################################################\n",
        "# 1st step: Training student net\n",
        "t_0 = time.time()\n",
        "iter_student = 0\n",
        "best_vloss = 1_000_000.\n",
        "training_logs_student = {\"train_student_loss\": [],  \"val_student_loss\": [], \"train_student_acc\": [], \"val_student_acc\": []}\n",
        "for epoch in range(num_epochs_student):\n",
        "    train_loss, train_correct = 0, 0\n",
        "    print(f'epochs {epoch+1:04d} / {num_epochs_student:04d}', end='\\n============\\n')\n",
        "\n",
        "    for i, data in enumerate(Dl_tar_training_set):\n",
        "    # Actually: (rely on number of data in Dataloader), for i, data in enumerate(trainloader, 0):\n",
        "        student_fn.train()\n",
        "        # zero parameters gradient to Net and optimizer\n",
        "        # model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # GPU processing source and target data\n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        yhat = student_fn(X)\n",
        "        loss = loss_fn(yhat, y)                 # source classification loss\n",
        "\n",
        "        # back-propagation (reversed grad to maximize domain loss)\n",
        "        loss.backward()\n",
        "        # optimization tuning\n",
        "        optimizer.step()\n",
        "\n",
        "        # #output predict from net\n",
        "        student_fn.eval()\n",
        "        with torch.no_grad():\n",
        "          class_prediction = student_fn(X)\n",
        "\n",
        "        print(f'[{i+1}/{len(Dl_tar_training_set)}] '\n",
        "              f'student class loss: {loss.item():.4f} '\n",
        "              )\n",
        "        # if batch_idx == 50:\n",
        "        #     print('This is just a demo, stopping...')\n",
        "        #     break\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_correct += (class_prediction.argmax(1) == y).float().sum().item()\n",
        "\n",
        "    training_logs_student[\"train_student_loss\"].append(train_loss / len(Dl_tar_training_set))\n",
        "    training_logs_student[\"train_student_acc\"].append(train_correct / len(Dl_tar_training_set.dataset))#.dataset))\n",
        "\n",
        "    running_vloss = 0.0\n",
        "    # Set the model to evaluation mode, disabling dropout and using population\n",
        "    # statistics for batch normalization.\n",
        "    student_fn.eval()\n",
        "    # Disable gradient computation and reduce memory consumption.\n",
        "    valid_loss, valid_correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for i, vdata in enumerate(Dl_tar_testing_set):\n",
        "            vinputs, vlabels = vdata[0].to(device), vdata[1].to(device)\n",
        "            voutputs = student_fn(vinputs)\n",
        "            vloss = loss_fn(voutputs, vlabels)\n",
        "            valid_loss += loss_fn(voutputs, vlabels).item()\n",
        "            valid_correct += (voutputs.argmax(1) == vlabels).float().sum().item()\n",
        "        # save testing logs\n",
        "        training_logs_student[\"val_student_loss\"].append(valid_loss / len(Dl_tar_testing_set))\n",
        "        training_logs_student[\"val_student_acc\"].append(valid_correct / len(Dl_tar_testing_set.dataset))\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        print(f\"Epochs {epoch+1}\".ljust(10),\n",
        "            f\"train loss {training_logs_student['train_student_loss'][-1]:.5f}\",\n",
        "            f\"train acc {training_logs_student['train_student_acc'][-1]:.5f}\",\n",
        "\n",
        "            f\"validate loss {training_logs_student['val_student_loss'][-1]:.5f}\",\n",
        "            f\"validate acc {training_logs_student['val_student_acc'][-1]:.5f}\",\n",
        "            )\n",
        "        print(\"-\"*80)\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if valid_loss < best_vloss:\n",
        "        best_vloss = valid_loss\n",
        "        # model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "        path_save_cp = './cp/'\n",
        "        if not os.path.exists(path_save_cp): os.mkdir(path_save_cp)\n",
        "        torch.save(student_fn.state_dict(), path_save_cp+'best_pruned_student_model.pth')\n",
        "\n",
        "    iter_student += 1\n",
        "\n",
        "t_end = time.time()-t_0\n",
        "print(f\"Time consumption for student net (device:{device}): {t_end} sec\")\n",
        "\n",
        "plot_graph(training_logs_student)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "VK_byU2cb4uJ",
        "outputId": "1ea3cbc6-60cc-4712-db67-74e98e584c42"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'function' object has no attribute 'nelement'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-8b4df20cf174>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#     print(\"-\"*80)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0minput_fp32\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_ptdq_int8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fp32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ptdq_int8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mcount_model_param_flops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_ptdq_int8\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_res\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiply_adds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-2b68ea26e2d8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool2d_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchnorm2d_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_cnn_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Flattening\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_lin1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#regular features classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1593\u001b[0m                         \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m                         \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-9cccecd85c83>\u001b[0m in \u001b[0;36mlinear_hook\u001b[0;34m(self, input, output)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mweight_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmultiply_adds\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mbias_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'nelement'"
          ]
        }
      ],
      "source": [
        "# PTDQ\n",
        "\n",
        "\n",
        "# quantized model instance\n",
        "student_fn.load_state_dict(torch.load('./cp/best_unquan_student_model.pth'))\n",
        "model_ptdq_int8 = torch.ao.quantization.quantize_dynamic(\n",
        "    student_fn,  # the original model\n",
        "    {torch.nn.Linear},  # a set of layers to dynamically quantize\n",
        "    dtype=torch.qint8)  # the target dtype for quantized weights\n",
        "\n",
        "# inferencing\n",
        "# valid_loss, valid_correct = 0, 0\n",
        "# student_ptdq_int8.eval()\n",
        "# with torch.no_grad():\n",
        "#     for i, vdata in enumerate(Dl_tar_testing_set):\n",
        "#         vinputs, vlabels = vdata[0].to(device), vdata[1].to(device)\n",
        "#         print(vinputs.shape)\n",
        "#         voutputs = student_ptdq_int8(vinputs)\n",
        "#         vloss = loss_fn(voutputs, vlabels)\n",
        "#         valid_loss += loss_fn(voutputs, vlabels).item()\n",
        "#         valid_correct += (voutputs.argmax(1) == vlabels).float().sum().item()\n",
        "#     # save testing logs\n",
        "#     training_logs_student[\"val_student_loss\"].append(valid_loss / len(Dl_tar_testing_set))\n",
        "#     training_logs_student[\"val_student_acc\"].append(valid_correct / len(Dl_tar_testing_set.dataset))\n",
        "\n",
        "# if epoch % 1 == 0:\n",
        "#     print(f\"Epochs {epoch+1}\".ljust(10),\n",
        "#         f\"validate loss {training_logs_student['val_student_loss'][-1]:.5f}\",\n",
        "#         f\"validate acc {training_logs_student['val_student_acc'][-1]:.5f}\",\n",
        "#         )\n",
        "#     print(\"-\"*80)\n",
        "input_fp32 = torch.randn(1, 3, 28, 28)\n",
        "res = model_ptdq_int8(input_fp32)\n",
        "summary(model_ptdq_int8, input_size=(channel_size, image_size, image_size))\n",
        "count_model_param_flops(model=model_ptdq_int8.eval(), input_res=28, multiply_adds=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kxQZ4kqlIfR"
      },
      "source": [
        "#### Non-quantized (FP32 training) to each quantization techniques\n",
        "\n",
        "Use the same network at model_qat_fp32 for base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_vyxdMgjZyg",
        "outputId": "93e0e9d8-47d0-4b65-c6e7-85cb740a68c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epochs 0001 / 0001\n",
            "============\n",
            "[1/29] student class loss: 2.3197 \n",
            "[2/29] student class loss: 2.2215 \n",
            "[3/29] student class loss: 2.1099 \n",
            "[4/29] student class loss: 2.0047 \n",
            "[5/29] student class loss: 1.8969 \n",
            "[6/29] student class loss: 1.8221 \n",
            "[7/29] student class loss: 1.7087 \n",
            "[8/29] student class loss: 1.6028 \n",
            "[9/29] student class loss: 1.5193 \n",
            "[10/29] student class loss: 1.4336 \n",
            "[11/29] student class loss: 1.3540 \n",
            "[12/29] student class loss: 1.2773 \n",
            "[13/29] student class loss: 1.2167 \n",
            "[14/29] student class loss: 1.1342 \n",
            "[15/29] student class loss: 1.0480 \n",
            "[16/29] student class loss: 0.9901 \n",
            "[17/29] student class loss: 0.9633 \n",
            "[18/29] student class loss: 0.8538 \n",
            "[19/29] student class loss: 0.8357 \n",
            "[20/29] student class loss: 0.7976 \n",
            "[21/29] student class loss: 0.7172 \n",
            "[22/29] student class loss: 0.7081 \n",
            "[23/29] student class loss: 0.6892 \n",
            "[24/29] student class loss: 0.6362 \n",
            "[25/29] student class loss: 0.5993 \n",
            "[26/29] student class loss: 0.6093 \n",
            "[27/29] student class loss: 0.5613 \n",
            "[28/29] student class loss: 0.5354 \n",
            "[29/29] student class loss: 0.5112 \n",
            "Epochs 1   train loss 1.19577 train acc 0.56633 validate loss 0.66804 validate acc 0.81824\n",
            "--------------------------------------------------------------------------------\n",
            "Time consumption for student net (device:cpu): 193.68452167510986 sec\n"
          ]
        }
      ],
      "source": [
        "# Optimizer and cost function\n",
        "optimizer = optim.Adam(model_qat_fp32.parameters(), lr)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "###########################################################################\n",
        "# 1st step: Training student net\n",
        "t_0 = time.time()\n",
        "iter_student = 0\n",
        "best_vloss = 1_000_000.\n",
        "training_logs_student = {\"train_student_loss\": [],  \"val_student_loss\": [], \"train_student_acc\": [], \"val_student_acc\": []}\n",
        "for epoch in range(num_epochs_student):\n",
        "    train_loss, train_correct = 0, 0\n",
        "    print(f'epochs {epoch+1:04d} / {num_epochs_student:04d}', end='\\n============\\n')\n",
        "\n",
        "    for i, data in enumerate(Dl_tar_training_set):\n",
        "    # Actually: (rely on number of data in Dataloader), for i, data in enumerate(trainloader, 0):\n",
        "        model_qat_fp32.train()\n",
        "        # zero parameters gradient to Net and optimizer\n",
        "        # model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # GPU processing source and target data\n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        yhat = model_qat_fp32(X)\n",
        "        loss = loss_fn(yhat, y)                 # source classification loss\n",
        "\n",
        "        # back-propagation (reversed grad to maximize domain loss)\n",
        "        loss.backward()\n",
        "        # optimization tuning\n",
        "        optimizer.step()\n",
        "\n",
        "        # #output predict from net\n",
        "        model_qat_fp32.eval()\n",
        "        with torch.no_grad():\n",
        "          class_prediction = model_qat_fp32(X)\n",
        "\n",
        "        print(f'[{i+1}/{len(Dl_tar_training_set)}] '\n",
        "              f'student class loss: {loss.item():.4f} '\n",
        "              )\n",
        "        # if batch_idx == 50:\n",
        "        #     print('This is just a demo, stopping...')\n",
        "        #     break\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_correct += (class_prediction.argmax(1) == y).float().sum().item()\n",
        "\n",
        "    training_logs_student[\"train_student_loss\"].append(train_loss / len(Dl_tar_training_set))\n",
        "    training_logs_student[\"train_student_acc\"].append(train_correct / len(Dl_tar_training_set.dataset))#.dataset))\n",
        "\n",
        "    running_vloss = 0.0\n",
        "    # Set the model to evaluation mode, disabling dropout and using population\n",
        "    # statistics for batch normalization.\n",
        "    model_qat_fp32.eval()\n",
        "    # Disable gradient computation and reduce memory consumption.\n",
        "    valid_loss, valid_correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for i, vdata in enumerate(Dl_tar_testing_set):\n",
        "            vinputs, vlabels = vdata[0].to(device), vdata[1].to(device)\n",
        "            voutputs = model_qat_fp32(vinputs)\n",
        "            vloss = loss_fn(voutputs, vlabels)\n",
        "            valid_loss += loss_fn(voutputs, vlabels).item()\n",
        "            valid_correct += (voutputs.argmax(1) == vlabels).float().sum().item()\n",
        "        # save testing logs\n",
        "        training_logs_student[\"val_student_loss\"].append(valid_loss / len(Dl_tar_testing_set))\n",
        "        training_logs_student[\"val_student_acc\"].append(valid_correct / len(Dl_tar_testing_set.dataset))\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        print(f\"Epochs {epoch+1}\".ljust(10),\n",
        "            f\"train loss {training_logs_student['train_student_loss'][-1]:.5f}\",\n",
        "            f\"train acc {training_logs_student['train_student_acc'][-1]:.5f}\",\n",
        "\n",
        "            f\"validate loss {training_logs_student['val_student_loss'][-1]:.5f}\",\n",
        "            f\"validate acc {training_logs_student['val_student_acc'][-1]:.5f}\",\n",
        "            )\n",
        "        print(\"-\"*80)\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if valid_loss < best_vloss:\n",
        "        best_vloss = valid_loss\n",
        "        # model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "        path_save_cp = './cp/'\n",
        "        if not os.path.exists(path_save_cp): os.mkdir(path_save_cp)\n",
        "        torch.save(student_fn.state_dict(), path_save_cp+'best_unquan_student_model.pth')\n",
        "\n",
        "    iter_student += 1\n",
        "\n",
        "t_end = time.time()-t_0\n",
        "print(f\"Time consumption for student net (device:{device}): {t_end} sec\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWrJDTZX2MK8",
        "outputId": "fa90544a-bd81-4573-fe6d-43cf040dbe8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test loss [PTDQ-int8-based student]: 2.30583\n",
            "test acc [PTDQ-int8-based student]: 0.08966\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "         QuantStub-1            [-1, 3, 28, 28]               0\n",
            "            Conv2d-2           [-1, 32, 24, 24]           2,432\n",
            "       BatchNorm2d-3           [-1, 32, 24, 24]              64\n",
            "              ReLU-4           [-1, 32, 24, 24]               0\n",
            "         MaxPool2d-5           [-1, 32, 12, 12]               0\n",
            "            Conv2d-6             [-1, 64, 8, 8]          51,264\n",
            "       BatchNorm2d-7             [-1, 64, 8, 8]             128\n",
            "              ReLU-8             [-1, 64, 8, 8]               0\n",
            "         MaxPool2d-9             [-1, 64, 4, 4]               0\n",
            "           Linear-10                   [-1, 64]               0\n",
            "             ReLU-11                   [-1, 64]               0\n",
            "           Linear-12                   [-1, 10]               0\n",
            "      DeQuantStub-13                   [-1, 10]               0\n",
            "       LogSoftmax-14                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 53,888\n",
            "Trainable params: 53,888\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.58\n",
            "Params size (MB): 0.21\n",
            "Estimated Total Size (MB): 0.79\n",
            "----------------------------------------------------------------\n",
            "Number of FLOPs: 0.009431 GFLOPs (9.43 MFLOPs)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor(9431104.)"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title PTDQ (model_qat_32 --> model_ptdq_int8)\n",
        "\n",
        "\n",
        "# quantized model instance\n",
        "model_qat_fp32.load_state_dict(torch.load('./cp/best_unquan_student_model.pth'))\n",
        "model_ptdq_int8 = torch.ao.quantization.quantize_dynamic(\n",
        "    model_qat_fp32,  # the original model\n",
        "    {torch.nn.Linear},  # a set of layers to dynamically quantize\n",
        "    dtype=torch.qint8)  # the target dtype for quantized weights\n",
        "\n",
        "# inferencing\n",
        "valid_loss, valid_correct = 0, 0\n",
        "with torch.no_grad():\n",
        "    for i, vdata in enumerate(Dl_tar_testing_set):\n",
        "        vinputs, vlabels = vdata[0].to(device), vdata[1].to(device)\n",
        "        voutputs = model_ptdq_int8(vinputs)\n",
        "        vloss = loss_fn(voutputs, vlabels)\n",
        "        valid_loss += loss_fn(voutputs, vlabels).item()\n",
        "        valid_correct += (voutputs.argmax(1) == vlabels).float().sum().item()\n",
        "    # save testing logs\n",
        "    print(f'test loss [PTDQ-int8-based student]: {valid_loss / len(Dl_tar_testing_set):.5f}'\n",
        "          f'\\ntest acc [PTDQ-int8-based student]: {valid_correct / len(Dl_tar_testing_set.dataset):.5f}')\n",
        "\n",
        "# input_fp32 = torch.randn(1, 3, 28, 28)\n",
        "# res = model_ptdq_int8(input_fp32)\n",
        "# print(f'tensor predicted: {res} || dtype: {res.dtype}')\n",
        "\n",
        "summary(model_ptdq_int8, input_size=(channel_size, image_size, image_size))\n",
        "count_model_param_flops(model=model_ptdq_int8.eval(), input_res=28, multiply_adds=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKtMvx3LlHQ5",
        "outputId": "fa21a9cc-be00-4c96-dafe-b9addf5878ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test loss [QAT-int8-based student]: 2.30259\n",
            "test acc [QAT-int8-based student]: 0.09754\n",
            "\n",
            "\n",
            "\n",
            "int8 ptsq model: qat_student_model(\n",
            "  (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
            "  (conv1): QuantizedConvReLU2d(3, 32, kernel_size=(5, 5), stride=(1, 1), scale=1.0, zero_point=0)\n",
            "  (batchnorm2d_1): Identity()\n",
            "  (relu_1): Identity()\n",
            "  (maxpool2d_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): QuantizedConvReLU2d(32, 64, kernel_size=(5, 5), stride=(1, 1), scale=1.0, zero_point=0)\n",
            "  (batchnorm2d_2): Identity()\n",
            "  (relu_2): Identity()\n",
            "  (maxpool2d_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (linear_1): QuantizedLinearReLU(in_features=1024, out_features=64, scale=1.0, zero_point=0, qscheme=torch.per_channel_affine)\n",
            "  (relu_lin1): Identity()\n",
            "  (linear_2): QuantizedLinear(in_features=64, out_features=10, scale=1.0, zero_point=0, qscheme=torch.per_channel_affine)\n",
            "  (logsoftmax): LogSoftmax(dim=1)\n",
            "  (dequant): DeQuantize()\n",
            ")\n",
            "\n",
            "\n",
            "\n",
            "Total parameters of Quantization-Aware Training (QAT)): 0\n",
            "Number of FLOPs: 0.000023 GFLOPs (0.02 MFLOPs)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "22528"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title PTDQ (model_qat_32 --> model_qat_int8)\n",
        "\n",
        "\n",
        "\n",
        "# model_qat_fp32 = qat_student_model()  # create a new model instance\n",
        "model_qat_fp32.load_state_dict(torch.load('./cp/best_unquan_student_model.pth'))  # load cp to model\n",
        "# model must be set to eval mode for static quantization logic to work\n",
        "model_qat_fp32.eval()\n",
        "\n",
        "# attach a global qconfig, which contains information about what kind\n",
        "# of observers to attach. Use 'x86' for server inference and 'qnnpack'\n",
        "# for mobile inference. Other quantization configurations such as selecting\n",
        "# symmetric or asymmetric quantization and MinMax or L2Norm calibration techniques\n",
        "# can be specified here.\n",
        "# Note: the old 'fbgemm' is still available but 'x86' is the recommended default\n",
        "# for server inference.\n",
        "# model_ptsq_fp32.qconfig = torch.ao.quantization.get_default_qconfig('fbgemm')\n",
        "model_qat_fp32.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\n",
        "# model_qat_fp32.qconfig = torch.ao.quantization.get_default_qconfig('qnnpack')\n",
        "\n",
        "# Fuse the activations to preceding layers, where applicable.\n",
        "# This needs to be done manually depending on the model architecture.\n",
        "# Common fusions include `conv + relu` and `conv + batchnorm + relu`\n",
        "model_qat_fp32_fused = torch.ao.quantization.fuse_modules(model_qat_fp32,\n",
        "                                                      [['conv1', 'batchnorm2d_1', 'relu_1'],\n",
        "                                                       ['conv2', 'batchnorm2d_2', 'relu_2'],\n",
        "                                                       ['linear_1', 'relu_lin1'],\n",
        "                                                      #  ['linear_2']\n",
        "                                                       ])\n",
        "\n",
        "# Prepare the model for static quantization. This inserts observers in\n",
        "# the model that will observe activation tensors during calibration.\n",
        "model_qat_fp32_prepared = torch.ao.quantization.prepare_qat(model_qat_fp32_fused.train())\n",
        "\n",
        "# Convert the observed model to a quantized model. This does several things:\n",
        "# quantizes the weights, computes and stores the scale and bias value to be\n",
        "# used with each activation tensor, and replaces key operators with quantized\n",
        "# implementations.\n",
        "model_qat_fp32_prepared.eval()\n",
        "model_qat_int8 = torch.ao.quantization.convert(model_qat_fp32_prepared)\n",
        "\n",
        "# run the model, relevant calculations will happen in int8\n",
        "# input_fp32 = torch.randn(1, 3, 28, 28)\n",
        "# res = model_qat_int8(input_fp32)\n",
        "# print(f'tensor predicted: {res} || dtype: {res.dtype}')\n",
        "\n",
        "valid_loss, valid_correct = 0, 0\n",
        "with torch.no_grad():\n",
        "    for i, vdata in enumerate(Dl_tar_testing_set):\n",
        "        vinputs, vlabels = vdata[0].to(device), vdata[1].to(device)\n",
        "        voutputs = model_qat_int8(vinputs)\n",
        "        vloss = loss_fn(voutputs, vlabels)\n",
        "        valid_loss += loss_fn(voutputs, vlabels).item()\n",
        "        valid_correct += (voutputs.argmax(1) == vlabels).float().sum().item()\n",
        "    # save testing logs\n",
        "    print(f'test loss [QAT-int8-based student]: {valid_loss / len(Dl_tar_testing_set):.5f}'\n",
        "          f'\\ntest acc [QAT-int8-based student]: {valid_correct / len(Dl_tar_testing_set.dataset):.5f}')\n",
        "\n",
        "\n",
        "# summary(model_int8, input_size=(channel_size, image_size, image_size))\n",
        "# Print the total number of parameters directly (no need for .numpy() anymore)\n",
        "print(f'\\n\\n\\nint8 ptsq model: {model_qat_int8}')\n",
        "total_params = sum(p.numel() for p in model_qat_int8.parameters())\n",
        "print(\"\\n\\n\\nTotal parameters of Quantization-Aware Training (QAT)):\", total_params)\n",
        "count_model_param_flops(model=model_qat_int8.eval(), input_res=28, multiply_adds=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxhQGXPMB6xU"
      },
      "source": [
        "# Summary\n",
        "Available techniques supported to some blocks/layers for doing dynamic and static quantization\n",
        "\n",
        "|Layers|Static Quantization|Dynamic Quantization|\n",
        "|-|-|-|\n",
        "|nn.Linear|Y|Y|\n",
        "|nn.Conv1d/2d/3d|Y|N|\n",
        "|nn.LSTM|N|Y|\n",
        "|nn.GRU|N|Y|\n",
        "|nn.RNNCell|N|Y|\n",
        "|nn.GRUCell|N|Y|\n",
        "|nn.LSTMCell|N|Y|\n",
        "|nn.EmbeddingBag|Y (activations are in fp32)|N|\n",
        "|nn.Embedding|Y|Y|\n",
        "|nn.MultiheadAttention|Not supported|Not supported|\n",
        "|Activations|Broadly supported|Un-changed, computations stay in fp32|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgnpElvVCken"
      },
      "source": [
        "# Homework\n",
        " quantization  VGG16 (Referred to W4-2) \n",
        "\n",
        "*   Non-quantized\n",
        "*   Dynamic quantization (PTDQ)\n",
        "*   Static quantization (PTSQ)\n",
        "*   Quantization-Aware Training (QAT)\n",
        "\n",
        "\n",
        " PDF  \n",
        "\n",
        "*   Quantitative results (acc., precision, recall, F1, confusion matrix, etc.)\n",
        "*   Computational resource in use (training time, parameters, FLOPs, etc.)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "VmAEKnLLqBhw"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
